---
title: "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning"

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - "Bei Ouyang*"
  - Shengyuan Ye*
  - Liekang Zeng
  - Tianyi Qian
  - Jingyi Li
  - Xu Chen

# Author notes (optional)
author_notes:
  - ""
  - ""
  - ""
  - ""
  - ""
  - ""

date: "2024-08-12"
doi: "https://doi.org/10.1145/3673038.3673043"

# Schedule page publish date (NOT publication's date).
publishDate: "2024-08-12"

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ["paper-conference"]

# Publication name and optional abbreviated publication name.
publication: In *International Conference on Parallel Processing*
publication_short: In *ICPP'24*

abstract: Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. Other studies focus on exploiting the potential of edge devices through resource management optimization, yet are ultimately bottlenecked by the resource wall of individual devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system codesign. (1) Algorithmically, PAC implements a personal LLMs finetuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone, enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64× end-to-end speedup and up to 88.16% reduction in memory footprint.

# Summary. An optional shortened abstract.
summary: Bei Ouyang*, Shengyuan Ye*, Liekang Zeng, Tianyi Qian, Jingyi Li, Xu Chen
# summary: "<u>Bei Ouyang</u>* , <u>Shengyuan Ye</u>* , Liekang Zeng, Tianyi Qian, Jingyi Li, Xu Chen"
# summary: "<b>Bei Ouyang</b>* , <b>Shengyuan Ye</b>* , Liekang Zeng, Tianyi Qian, Jingyi Li, Xu Chen<sup>&#8224;</sup>"

tags:
  - Large Language Models
  - Edge intelligence

# Display this page in the Featured widget? Featured Publications
# featured: true 使得页面可以在“Featured”小部件中显示。如果你希望某些内容在网站上被突出展示，通过设置这个属性可以将它们放在更显眼的位置。
featured: true

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: "publication/2024_pac/PAC_paper.pdf"
url_code: ""
url_dataset: ""
url_poster: ""
url_project: ""
url_slides: "publication/2024_pac/PAC_slides.pdf"
url_source: ""
url_video: ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: "Image PAC"
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
  # - example

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: #example
---

<!-- {{% callout note %}}
Click the _Cite_ button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

{{% callout note %}}
Create your slides in Markdown - click the _Slides_ button to check out the example.
{{% /callout %}}

Add the publication's **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). -->

<!-- **Equal contribution:** Bei Ouyang, Shengyuan Ye
**Corresponding author:** Xu Chen -->
